{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZSXKRi00W-sU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d1f585-39a5-48db-caff-5a7d9eb6e95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings\n",
        "Each word in the input sequence would have a vector of embedding_dimension of 512.\n",
        "\n",
        "Output dimension = (batch_size, sequence_length, embedding_dimension)\n",
        "Word_Embedding_Matrix dimension = (Vocab_length, embedding_dimension)"
      ],
      "metadata": {
        "id": "P7GJmTkgAWVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self,vocab_size,embedding_dim):\n",
        "        super(Embedding,self).__init__()\n",
        "        self.embed=nn.Embedding(vocab_size,embedding_dim)\n",
        "    def forward(self,x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "HNuD3jXCENX0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embeddings\n",
        "Dimensions are similar to Word Embeddings\n",
        "Later the outputs of these two layers are added"
      ],
      "metadata": {
        "id": "MQd3R-QdFFCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,embedding_dim):\n",
        "        super(PositionalEmbedding,self).__init__()\n",
        "        self.embedding_dim=embedding_dim\n",
        "        pe =torch.zeros(max_seq_len,embedding_dim)\n",
        "        # pos -> refers to order in the sentence\n",
        "        # i -> refers to position along embedding vector dimension (i-even,i+1-odd)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0,embedding_dim,2):\n",
        "                pe[pos,i]=math.sin(pos/(10000**((2*i)/embedding_dim)))\n",
        "                pe[pos,i+1]=math.cos(pos/(10000**((2*i)/embedding_dim)))\n",
        "        pe=pe.unsqueeze(0) #\n",
        "        self.register_buffer('positional_embedding',pe)\n",
        "        #register_buffer ---> stored in state_dict but non-trainable\n",
        "    def forward(self,x):\n",
        "        x=x*math.sqrt(self.embedding_dim)\n",
        "        seq_len=x.size(1) #current seq_len of the word instead of max_sequnece_length\n",
        "        x=x+torch.autograd.Variable(self.pe[:,:seq_len,])\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "XUBdn0oaFEyq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "KTKFv2crOZVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q_matrix,\n",
        "K_matrix,\n",
        "V_matrix"
      ],
      "metadata": {
        "id": "NfrMfmTLQDQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,embedding_dim=512,num_heads=8):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.head_dim=embedding_dim//num_heads # 512/8 = 64\n",
        "        #ALL the QKV matrices are 64,64\n",
        "        self.key_matrix = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "        self.query_matrix = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "        self.value_matrix = nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "        self.out = nn.Linear(self.num_heads * self.head_dim,self.embedding_dim) #512 * 512\n",
        "    def forward(self,key,query,value,mask=None):\n",
        "        batch_size=key.size(0)\n",
        "        seq_length=key.size(1)\n",
        "        # query dimension can change in decoder during inference.\n",
        "        # so we cant take general seq_length\n",
        "        seq_length_query = query.size(1)\n",
        "        key = key.view(batch_size,seq_length,self.n_heads,self.head_dim)\n",
        "        #i.e 32,10,512 ---> 32,10,8,64\n",
        "        query = query.view(batch_size,seq_length_query,self.n_heads,self.head_dim)\n",
        "        #i.e 32,seq_len_query,512 ---> 32,seq_len_query,8,64\n",
        "        value = value.view(batch_size,seq_length,self.n_heads,self.head_dim)\n",
        "        #i.e 32,10,512 ---> 32,10,8,64\n",
        "        k=self.key_matrix(key) # 64*64 x 32,10,8,64\n",
        "        q=self.query_matrix(query)\n",
        "        v=self.value_matrix(value)\n",
        "\n",
        "        q=q.transpose(1,2) #32,8,10,64\n",
        "        k=k.transpose(1,2) #32,8,10,64\n",
        "        v=v.transpose(1,2) #32,8,10,64 each head has 10 seq len vectors each of 64 dimensional vector(each word has 64dim vector)\n",
        "\n",
        "        # computes attention (QK.T)/d^1/2\n",
        "        # adjust key for matrix multiplication\n",
        "        k_adjusted = k.transpose(-1,-2) # 32,8,64,10\n",
        "        product = torch.matmul(q, k_adjusted) #32,8,10,10\n",
        "\n",
        "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
        "        if mask is not None:\n",
        "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        #divising by square root of key dimension\n",
        "        product = product / math.sqrt(self.single_head_dim) #  sqrt(64) ~ 8\n",
        "\n",
        "        #applying softmax\n",
        "        scores = F.softmax(product, dim=-1) #32,8,10,10 last layer is normalized\n",
        "        scores = torch.matmul(scores, v)  # (32,8,10,10) x (32,8,10,64) = (32,8,10,64)\n",
        "        scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32,8,10,64)  -> (32,10,8,64)   -> (32,10,512)\n",
        "        #contiguous becuase the memory copy is no stored as contiguos memeory\n",
        "        output = self.out(scores)  # (32,10,512) -> (32,10,512)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "dBenI1YELFyA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2,3,4)\n",
        "print(a)\n",
        "b=a.view(2,3,2,2)\n",
        "print(b)\n",
        "c=b.transpose(1,2)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHyFURJGiUUt",
        "outputId": "35d5ba83-f80e-4db1-ab0c-46d46af440b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0076, 0.7412, 0.3044, 0.6124],\n",
            "         [0.6822, 0.0135, 0.4996, 0.0588],\n",
            "         [0.5986, 0.3923, 0.6880, 0.0489]],\n",
            "\n",
            "        [[0.7553, 0.7426, 0.1792, 0.7642],\n",
            "         [0.5237, 0.2528, 0.6227, 0.2845],\n",
            "         [0.4573, 0.2274, 0.1588, 0.6087]]])\n",
            "tensor([[[[0.0076, 0.7412],\n",
            "          [0.3044, 0.6124]],\n",
            "\n",
            "         [[0.6822, 0.0135],\n",
            "          [0.4996, 0.0588]],\n",
            "\n",
            "         [[0.5986, 0.3923],\n",
            "          [0.6880, 0.0489]]],\n",
            "\n",
            "\n",
            "        [[[0.7553, 0.7426],\n",
            "          [0.1792, 0.7642]],\n",
            "\n",
            "         [[0.5237, 0.2528],\n",
            "          [0.6227, 0.2845]],\n",
            "\n",
            "         [[0.4573, 0.2274],\n",
            "          [0.1588, 0.6087]]]])\n",
            "tensor([[[[0.0076, 0.7412],\n",
            "          [0.6822, 0.0135],\n",
            "          [0.5986, 0.3923]],\n",
            "\n",
            "         [[0.3044, 0.6124],\n",
            "          [0.4996, 0.0588],\n",
            "          [0.6880, 0.0489]]],\n",
            "\n",
            "\n",
            "        [[[0.7553, 0.7426],\n",
            "          [0.5237, 0.2528],\n",
            "          [0.4573, 0.2274]],\n",
            "\n",
            "         [[0.1792, 0.7642],\n",
            "          [0.6227, 0.2845],\n",
            "          [0.1588, 0.6087]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENCODER"
      ],
      "metadata": {
        "id": "uo8eUTj3h_2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self,embedding_dim,expansion_factor=4,num_heads=8):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.feed_forward=nn.Sequential(\n",
        "            nn.Linear(embedding_dim,expansion_factor*embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(expansion_factor*embedding_dim,embedding_dim)\n",
        "        ) #this is like expanding into more dimensional and reducing back to original dimension\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "    def forward(self,key,query,output):\n",
        "        attention_ouput=self.attention(key,query,output)\n",
        "        attention_residual=attention_ouput+output\n",
        "        x=self.dropout1(self.norm1(attention_residual))\n",
        "        feed_forward_output=self.feed_forward(x)\n",
        "        feed_forward_residual=feed_forward_output+x\n",
        "        output=self.dropout2(self.norm2(feed_forward_residual))\n",
        "        return output\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self,seq_len,vocab_size,embedding_dim,num_layers=2,expansion_factor=4,num_heads=8):\n",
        "        super(TransformerEncoder,self).__init__()\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.positional_encoder = PositionalEmbedding(seq_len, embedding_dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embedding_dim, expansion_factor, num_heads) for _ in range(num_layers)])\n",
        "    def forward(self,x):\n",
        "        embed_out = self.embedding_layer(x)\n",
        "        out = self.positional_encoder(embed_out)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f620qPKQiCGC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder\n",
        "In Decoder during cross attention key,value vectors are from Encoder Output whereas Query is from previous decoder block\n",
        "Also masking is applied during training."
      ],
      "metadata": {
        "id": "H-glwFgzS4rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,embedding_dim,expansion_factor=4,num_heads=8):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.transfomer_block = TransformerBlock(embedding_dim, expansion_factor, num_heads)\n",
        "    def forward(self,key,query,x,mask):\n",
        "        #we need to pass mask to only the first attention\n",
        "        attention = self.attention(x,x,x,mask=mask) #32x10x512\n",
        "        value = self.dropout(self.norm(attention + x))\n",
        "        out = self.transformer_block(key, query, value)\n",
        "        return out\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,target_vocab_size,embedding_dim,seq_len, num_layers=2, expansion_factor=4, num_heads=8):\n",
        "        super(TransformerDecoder,self).__init__()\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.positional_encoder = PositionalEmbedding(seq_len, embedding_dim)\n",
        "        self.layers =nn.ModuleList([DecoderBlock(embedding_dim, expansion_factor, num_heads) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(embedding_dim, target_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    def forward(self,x,enc_out,mask):\n",
        "        x = self.word_embedding(x)  #32x10x512\n",
        "        x = self.position_embedding(x) #32x10x512\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, x, enc_out, mask)  #key,query,x\n",
        "\n",
        "        out = F.softmax(self.fc_out(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "A08PmkahS6o4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, num_heads=8):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embedding_dim, num_layers=num_layers, expansion_factor=expansion_factor, num_heads=num_heads)\n",
        "        self.decoder = TransformerDecoder(target_vocab_size, embedding_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, num_heads=num_heads)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        # returns the lower triangular part of matrix filled with ones\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            batch_size, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask\n",
        "\n",
        "    def decode(self,src,trg):\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "        out_labels = []\n",
        "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
        "        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
        "        out = trg\n",
        "        for i in range(seq_len): #10\n",
        "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
        "            # taking the last token\n",
        "            out = out[:,-1,:]\n",
        "\n",
        "            out = out.argmax(-1)\n",
        "            out_labels.append(out.item())\n",
        "            out = torch.unsqueeze(out,axis=0)\n",
        "        return out_labels\n",
        "    def forward(self, src, trg):\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encoder(src)\n",
        "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Vld1VNbmTUAk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 11\n",
        "target_vocab_size = 11\n",
        "num_layers = 6\n",
        "seq_length= 12\n",
        "\n",
        "\n",
        "# let 0 be sos token and 1 be eos token\n",
        "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
        "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
        "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
        "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
        "\n",
        "print(src.shape,target.shape)\n",
        "model = Transformer(embedding_dim=512, src_vocab_size=src_vocab_size,target_vocab_size=target_vocab_size, seq_length=seq_length,num_layers=num_layers, expansion_factor=4, num_heads=8)\n",
        "model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LDSClzmddQv",
        "outputId": "ecb0ba6a-e2aa-406e-bf88-40be0d78999b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12]) torch.Size([2, 12])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (positional_encoder): PositionalEmbedding()\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): TransformerDecoder(\n",
              "    (positional_encoder): PositionalEmbedding()\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (transfomer_block): TransformerBlock(\n",
              "          (attention): MultiHeadAttention(\n",
              "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Sequential(\n",
              "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.2, inplace=False)\n",
              "          (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=512, out_features=11, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}